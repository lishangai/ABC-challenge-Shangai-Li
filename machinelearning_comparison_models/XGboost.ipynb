{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoader\n",
    "from feature_extractor import FeatureExtractor\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import time\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import  KFold,cross_val_score,cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        初始化模型训练器（XGBoost专用版）\n",
    "        \"\"\"\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()  # 新增标签编码器\n",
    "        self.classes_ = None  # 保存原始类别标签\n",
    "\n",
    "    def prepare_data(self, features, labels):\n",
    "        \"\"\"\n",
    "        准备训练数据（集成标签编码）\n",
    "        \"\"\"\n",
    "        # 编码标签为0开始的整数\n",
    "        y_encoded = self.label_encoder.fit_transform(labels)\n",
    "        self.classes_ = self.label_encoder.classes_  # 保存原始标签\n",
    "\n",
    "        # 划分训练集和测试集（分层抽样使用编码后标签）\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features, y_encoded,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=y_encoded\n",
    "        )\n",
    "        # 标准化特征\n",
    "        X_train = self.scaler.fit_transform(X_train)\n",
    "        X_test = self.scaler.transform(X_test)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        构建优化版XGBoost模型（新增早停相关参数）\n",
    "        \"\"\"\n",
    "        self.model = XGBClassifier(\n",
    "            n_estimators=1000,  # 设置更大的树数量以支持早停\n",
    "            max_depth=5,  # 降低树深度\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_alpha=0.2,\n",
    "            reg_lambda=1.0,\n",
    "            objective='multi:softmax',  # 使用softmax更直观\n",
    "            num_class=len(self.classes_),\n",
    "            tree_method='hist',\n",
    "            eval_metric='mlogloss',  # 明确评估指标\n",
    "            early_stopping_rounds=50,  # 新增早停参数\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        训练XGBoost模型（集成早停和评估）\n",
    "        \"\"\"\n",
    "        # 分割验证集\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train,\n",
    "            test_size=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        # 计算样本权重\n",
    "        sample_weights = compute_sample_weight('balanced', y_train)\n",
    "\n",
    "        # 训练模型（带早停）\n",
    "        self.model.fit(\n",
    "            X_train, y_train,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=50  # 每50轮打印进度\n",
    "        )\n",
    "\n",
    "        # 训练结果分析\n",
    "        print(\"\\nBest iteration:\", self.model.best_iteration)\n",
    "        print(\"Best validation score: {:.3f}\".format(self.model.best_score))\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        评估模型（支持原始标签显示）\n",
    "        \"\"\"\n",
    "        # 转换预测结果到原始标签\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_test_decoded = self.label_encoder.inverse_transform(y_test)\n",
    "        y_pred_decoded = self.label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "        # 生成分类报告\n",
    "        class_report = classification_report(\n",
    "            y_test_decoded, y_pred_decoded,\n",
    "            target_names=self.classes_.astype(str)\n",
    "        )\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(class_report)\n",
    "\n",
    "        # 绘制混淆矩阵（原始标签）\n",
    "        conf_matrix = confusion_matrix(y_test_decoded, y_pred_decoded)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(\n",
    "            conf_matrix,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='Blues',\n",
    "            xticklabels=self.classes_,\n",
    "            yticklabels=self.classes_\n",
    "        )\n",
    "        plt.title('Confusion Matrix - XGBoost')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.savefig('xgboost_confusion_matrix.png')\n",
    "        plt.close()\n",
    "\n",
    "        # 特征重要性分析\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': [f'feature_{i}' for i in range(X_test.shape[1])],\n",
    "            'importance': self.model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_importance.head(15))\n",
    "        plt.title('Top 15 Important Features (XGBoost)')\n",
    "        plt.savefig('xgboost_feature_importance.png')\n",
    "        plt.close()\n",
    "\n",
    "        return class_report\n",
    "\n",
    "    def save_model(self, model_path='xgboost_model.pkl',\n",
    "                   scaler_path='scaler.pkl',\n",
    "                   encoder_path='label_encoder.pkl'):\n",
    "        \"\"\"\n",
    "        保存完整模型体系\n",
    "        \"\"\"\n",
    "        joblib.dump(self.model, model_path)\n",
    "        joblib.dump(self.scaler, scaler_path)\n",
    "        joblib.dump(self.label_encoder, encoder_path)\n",
    "        print(f\"Model artifacts saved to {model_path}, {scaler_path}, {encoder_path}\")\n",
    "\n",
    "    def load_model(self, model_path='xgboost_model.pkl',\n",
    "                   scaler_path='scaler_XGboost.pkl',\n",
    "                   encoder_path='label_encoder_XGboost.pkl'):\n",
    "        \"\"\"\n",
    "        加载完整模型体系\n",
    "        \"\"\"\n",
    "        self.model = joblib.load(model_path)\n",
    "        self.scaler = joblib.load(scaler_path)\n",
    "        self.label_encoder = joblib.load(encoder_path)\n",
    "        self.classes_ = self.label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Valid activity types from TrainActivities.csv:\n",
      "ID 2806: 1 (FACING camera) Sit and stand\n",
      "ID 2807: 2 (FACING camera) both hands SHAKING (sitting position)\n",
      "ID 2808: 3 Stand up from chair - both hands with SHAKING\n",
      "ID 2809: 4 (Sideway) Sit & stand\n",
      "ID 2810: 5 (Sideway) both hands SHAKING (sitting)\n",
      "ID 2811: 6 (Sideway) STAND up with - both hands SHAKING\n",
      "ID 2812: 7 Cool down - sitting/relax\n",
      "ID 2813: 8 Walk (LEFT --> Right --> Left)\n",
      "ID 2814: 9 Walk & STOP/frozen, full body shaking, rotate then return back\n",
      "ID 2815: 10 Slow walk (SHAKING hands/body, tiny step, head forward)\n",
      "Loading accelerometer data...\n",
      "\n",
      "Found 75 CSV files in valid activity folders\n",
      "\n",
      "Processing activity type 2806...\n",
      "Processed 10/75 files\n",
      "\n",
      "Processing activity type 2807...\n",
      "\n",
      "Processing activity type 2808...\n",
      "Processed 20/75 files\n",
      "\n",
      "Processing activity type 2809...\n",
      "\n",
      "Processing activity type 2810...\n",
      "Processed 30/75 files\n",
      "\n",
      "Processing activity type 2811...\n",
      "\n",
      "Processing activity type 2812...\n",
      "Processed 40/75 files\n",
      "\n",
      "Processing activity type 2813...\n",
      "Processed 50/75 files\n",
      "\n",
      "Processing activity type 2814...\n",
      "Processed 60/75 files\n",
      "\n",
      "Processing activity type 2815...\n",
      "Processed 70/75 files\n",
      "\n",
      "Merging all data...\n",
      "\n",
      "Data loading completed:\n",
      "Total samples: 141325\n",
      "Activity types found: [np.int64(2806), np.int64(2807), np.int64(2808), np.int64(2809), np.int64(2810), np.int64(2811), np.int64(2812), np.int64(2813), np.int64(2814), np.int64(2815)]\n",
      "Files per activity type:\n",
      "Activity Type ID\n",
      "2806    11\n",
      "2807     8\n",
      "2808     6\n",
      "2809     2\n",
      "2810     5\n",
      "2811     7\n",
      "2812     9\n",
      "2813     8\n",
      "2814    10\n",
      "2815     9\n",
      "Name: Source_File, dtype: int64\n",
      "Total files processed: 75\n",
      "\n",
      "Extracting features...\n",
      "\n",
      "Segmentation summary:\n",
      "Total segments: 4316\n",
      "Activity 2806: 680 segments\n",
      "Activity 2807: 412 segments\n",
      "Activity 2808: 296 segments\n",
      "Activity 2809: 49 segments\n",
      "Activity 2810: 183 segments\n",
      "Activity 2811: 287 segments\n",
      "Activity 2812: 360 segments\n",
      "Activity 2813: 301 segments\n",
      "Activity 2814: 884 segments\n",
      "Activity 2815: 864 segments\n",
      "\n",
      "Feature extraction:\n",
      "Processing segment 1/4316\n",
      "Processing segment 101/4316\n",
      "Processing segment 201/4316\n",
      "Processing segment 301/4316\n",
      "Processing segment 401/4316\n",
      "Processing segment 501/4316\n",
      "Processing segment 601/4316\n",
      "Processing segment 701/4316\n",
      "Processing segment 801/4316\n",
      "Processing segment 901/4316\n",
      "Processing segment 1001/4316\n",
      "Processing segment 1101/4316\n",
      "Processing segment 1201/4316\n",
      "Processing segment 1301/4316\n",
      "Processing segment 1401/4316\n",
      "Processing segment 1501/4316\n",
      "Processing segment 1601/4316\n",
      "Processing segment 1701/4316\n",
      "Processing segment 1801/4316\n",
      "Processing segment 1901/4316\n",
      "Processing segment 2001/4316\n",
      "Processing segment 2101/4316\n",
      "Processing segment 2201/4316\n",
      "Processing segment 2301/4316\n",
      "Processing segment 2401/4316\n",
      "Processing segment 2501/4316\n",
      "Processing segment 2601/4316\n",
      "Processing segment 2701/4316\n",
      "Processing segment 2801/4316\n",
      "Processing segment 2901/4316\n",
      "Processing segment 3001/4316\n",
      "Processing segment 3101/4316\n",
      "Processing segment 3201/4316\n",
      "Processing segment 3301/4316\n",
      "Processing segment 3401/4316\n",
      "Processing segment 3501/4316\n",
      "Processing segment 3601/4316\n",
      "Processing segment 3701/4316\n",
      "Processing segment 3801/4316\n",
      "Processing segment 3901/4316\n",
      "Processing segment 4001/4316\n",
      "Processing segment 4101/4316\n",
      "Processing segment 4201/4316\n",
      "Processing segment 4301/4316\n",
      "\n",
      "Features shape: (4316, 33)\n"
     ]
    }
   ],
   "source": [
    "# 初始化数据加载器\n",
    "data_loader = DataLoader(\n",
    "    base_dir='activity_segments',\n",
    "    activity_file='TrainActivities.csv'\n",
    ")\n",
    "\n",
    "# 加载数据\n",
    "print(\"Loading accelerometer data...\")\n",
    "data = data_loader.load_data()\n",
    "\n",
    "# 特征提取\n",
    "print(\"\\nExtracting features...\")\n",
    "feature_extractor = FeatureExtractor(window_size=64, overlap=0.5)\n",
    "segments, labels = feature_extractor.segment_data(data)\n",
    "\n",
    "print(f\"\\nFeature extraction:\")\n",
    "# 从所有段中提取特征\n",
    "features = []\n",
    "for i, segment in enumerate(segments):\n",
    "    if i % 100 == 0:  # 每处理100个片段打印一次进度\n",
    "        print(f\"Processing segment {i+1}/{len(segments)}\")\n",
    "    segment_features = feature_extractor.extract_features(segment)\n",
    "    features.append(list(segment_features.values()))\n",
    "\n",
    "# 转换为numpy数组\n",
    "features = np.array(features)\n",
    "print(f\"\\nFeatures shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:2.20054\n",
      "[50]\tvalidation_0-mlogloss:1.21848\n",
      "[100]\tvalidation_0-mlogloss:1.09568\n",
      "[150]\tvalidation_0-mlogloss:1.04707\n",
      "[200]\tvalidation_0-mlogloss:1.02348\n",
      "[250]\tvalidation_0-mlogloss:1.01727\n",
      "[300]\tvalidation_0-mlogloss:1.01113\n",
      "[350]\tvalidation_0-mlogloss:1.00982\n",
      "[368]\tvalidation_0-mlogloss:1.01376\n",
      "\n",
      "Best iteration: 318\n",
      "Best validation score: 1.007\n"
     ]
    }
   ],
   "source": [
    "# 训练XGBoost模型\n",
    "model_trainer = ModelTrainer()\n",
    "X_train, X_test, y_train, y_test = model_trainer.prepare_data(features, labels)\n",
    "\n",
    "# 记录训练开始时间\n",
    "start_time = time.time()\n",
    "\n",
    "model_trainer.build_model()\n",
    "model_trainer.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Time: 6.04 seconds\n",
      "\n",
      "Model Evaluation Results:\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        2806       0.62      0.71      0.66       136\n",
      "        2807       0.63      0.58      0.60        83\n",
      "        2808       0.65      0.51      0.57        59\n",
      "        2809       0.50      0.50      0.50        10\n",
      "        2810       0.69      0.65      0.67        37\n",
      "        2811       0.71      0.47      0.57        57\n",
      "        2812       0.59      0.57      0.58        72\n",
      "        2813       0.59      0.60      0.60        60\n",
      "        2814       0.58      0.68      0.63       177\n",
      "        2815       0.69      0.67      0.68       173\n",
      "\n",
      "    accuracy                           0.63       864\n",
      "   macro avg       0.63      0.59      0.61       864\n",
      "weighted avg       0.63      0.63      0.63       864\n",
      "\n",
      "\n",
      "Saving model and results...\n",
      "Model artifacts saved to saved_model.pkl, saved_scaler.pkl, label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# 计算训练时间\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: {training_time:.2f} seconds\")\n",
    "\n",
    "# 评估模型\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "evaluation_results = model_trainer.evaluate(X_test, y_test)\n",
    "\n",
    "# 保存模型和评估结果\n",
    "print(\"\\nSaving model and results...\")\n",
    "model_trainer.save_model('saved_model.pkl', 'saved_scaler.pkl')\n",
    "\n",
    "# 保存结果到文件\n",
    "with open('model_results.txt', 'w') as f:\n",
    "    f.write(\"Random Forest Model Results\\n\")\n",
    "    f.write(\"==========================\\n\\n\")\n",
    "    f.write(f\"Training Time: {training_time:.2f} seconds\\n\\n\")\n",
    "    f.write(\"Evaluation Results:\\n\")\n",
    "    f.write(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习曲线\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import  KFold, GridSearchCV,HalvingGridSearchCV, cross_validate, RandomizedSearchCV\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "\n",
    "#划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = model_trainer.prepare_data(features, labels)\n",
    "\n",
    "label_encoder = LabelEncoder()  # 新增标签编码器\n",
    "# 编码标签为0开始的整数\n",
    "y_encoded = label_encoder.fit_transform(labels)\n",
    "classes_ = label_encoder.classes_  # 保存原始标签\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以学习率为例子的学习曲线图像\n",
    "tr = []\n",
    "te = []\n",
    "\n",
    "# for i in np.arange(0,1,0.25):\n",
    "xgb1 = XGBClassifier(n_estimators=1000,  # 设置更大的树数量以支持早停\n",
    "                    max_depth=5,  # 降低树深度\n",
    "                    learning_rate=0.1,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    reg_alpha=0.2,\n",
    "                    reg_lambda=1.0,\n",
    "                    objective='multi:softmax',  # 使用softmax更直观\n",
    "                    num_class=9,\n",
    "                    tree_method='hist',\n",
    "                    eval_metric='mlogloss',  # 明确评估指标\n",
    "                    # early_stopping_rounds=50,  # 新增早停参数\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42,\n",
    "                    )\n",
    "core_te = cross_val_score(xgb1,features,y_encoded,cv=4).mean() # 4折交叉验证的准确率\n",
    "\n",
    "# tr.append(score_tr)\n",
    "# te.append(score_te)\n",
    "# plt.plot(np.arange(0,1,0.1),tr,color=\"red\",label=\"train\")\n",
    "# plt.plot(np.arange(0,1,0.1),te,color=\"blue\",label=\"test\")\n",
    "# plt.xticks(np.arange(0,1,0.1))\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.2704480103696229)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0]\n",
      "[np.float64(0.2636062664294138)]\n"
     ]
    }
   ],
   "source": [
    "print(tr)\n",
    "print(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.58197031, -0.76946175, -0.9407475 , ..., -0.02736846,\n",
       "        -0.44712789, -0.96073682],\n",
       "       [-0.49687715, -0.31147539, -0.51269575, ...,  1.57349504,\n",
       "         0.79219401,  2.52492714],\n",
       "       [ 0.21717143,  0.13562872,  0.09257455, ...,  0.19386117,\n",
       "        -0.34626803, -0.08932083],\n",
       "       ...,\n",
       "       [ 0.92274524,  1.59025409,  1.78852973, ..., -0.55054167,\n",
       "        -0.56478409,  0.25924557],\n",
       "       [ 1.45187823,  1.45905125,  1.86388074, ..., -0.09235759,\n",
       "        -0.65049665,  0.43352877],\n",
       "       [-0.40342184,  1.85768806,  0.91372136, ...,  0.08082716,\n",
       "        -0.48166938,  0.43352877]], shape=(790, 33))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
