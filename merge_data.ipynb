{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成，总数据行数: 2045326\n",
      "其中有效时间戳数量: 2045326\n",
      "有效Minute数量: 2045326\n"
     ]
    }
   ],
   "source": [
    "# 路径设置\n",
    "base_dir = \"users_timeXYZ/users/\"  # 加速度计数据的主目录\n",
    "activity_file = \"TrainActivities.csv\"  # 活动标签文件\n",
    "\n",
    "# 初始化空的 DataFrame 用于存储所有加速度数据\n",
    "all_accel_data = pd.DataFrame()\n",
    "\n",
    "# 遍历所有子文件夹\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if os.path.isdir(folder_path):  # 确保是文件夹\n",
    "        # 遍历文件夹中的所有 CSV 文件\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".csv\"):  # 确保是 CSV 文件\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                # 读取 CSV 文件并手动指定列名\n",
    "                data = pd.read_csv(\n",
    "                    file_path, \n",
    "                    header=None,  # 因为文件没有列名\n",
    "                    names=[\"RandomID\", \"Timestamp\", \"Accel_X\", \"Accel_Y\", \"Accel_Z\"]  # 手动指定列名\n",
    "                )\n",
    "                \n",
    "                # 1. 去除加速度数据中的缺失值\n",
    "                data = data.dropna(subset=[\"Timestamp\", \"Accel_X\", \"Accel_Y\", \"Accel_Z\"])\n",
    "                \n",
    "                # 2. 选择有用的列（忽略 RandomID 列）\n",
    "                data = data[[\"Timestamp\", \"Accel_X\", \"Accel_Y\", \"Accel_Z\"]]\n",
    "                \n",
    "                # 3. 将时间戳转换为标准格式，去除无效时间戳\n",
    "                data[\"Timestamp\"] = pd.to_datetime(\n",
    "                    data[\"Timestamp\"], \n",
    "                    format=\"%Y-%m-%dT%H:%M:%S.%f%z\",  # 解析时间戳样式\n",
    "                    errors=\"coerce\"  # 无法解析的时间戳设置为 NaT\n",
    "                )\n",
    "                \n",
    "                # 4. 去除转换后无效的时间戳数据\n",
    "                data = data.dropna(subset=[\"Timestamp\"])\n",
    "                \n",
    "                # 5. 创建 'Minute' 列，提取时间戳的分钟部分\n",
    "                data[\"Minute\"] = data[\"Timestamp\"].dt.floor(\"min\")\n",
    "                \n",
    "                # 6. 添加文件来源信息\n",
    "                data[\"SourceFile\"] = file\n",
    "                \n",
    "                # 7. 只保留有效数据\n",
    "                if len(data) > 0:  # 确保还有数据\n",
    "                    all_accel_data = pd.concat([all_accel_data, data], ignore_index=True)\n",
    "\n",
    "print(f\"处理完成，总数据行数: {len(all_accel_data)}\")\n",
    "print(f\"其中有效时间戳数量: {all_accel_data['Timestamp'].notna().sum()}\")\n",
    "print(f\"有效Minute数量: {all_accel_data['Minute'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间戳范围（UTC）：\n",
      "最早时间：2024-09-01 21:25:07.752000+00:00\n",
      "最晚时间：2024-09-12 20:19:47.443000+00:00\n",
      "\n",
      "总数据行数: 2045326\n"
     ]
    }
   ],
   "source": [
    "# 先将 Timestamp 转成 datetime 并转换到UTC\n",
    "all_accel_data[\"Timestamp\"] = pd.to_datetime(\n",
    "    all_accel_data[\"Timestamp\"],\n",
    "    format=\"%Y-%m-%dT%H:%M:%S.%f%z\",  # 解析带时区的时间戳\n",
    "    utc=True  # 转换到UTC\n",
    ")\n",
    "\n",
    "# 创建Minute列（向下取整到分钟）\n",
    "all_accel_data[\"Minute\"] = all_accel_data[\"Timestamp\"].dt.floor(\"min\")\n",
    "\n",
    "# 按时间戳排序\n",
    "all_accel_data = all_accel_data.sort_values(by=\"Timestamp\").reset_index(drop=True)\n",
    "\n",
    "# 保存预览数据\n",
    "all_accel_data.to_csv(\"all_accel_data_preview.csv\", index=False)\n",
    "\n",
    "# 打印一些信息来验证\n",
    "print(\"时间戳范围（UTC）：\")\n",
    "print(f\"最早时间：{all_accel_data['Timestamp'].min()}\")\n",
    "print(f\"最晚时间：{all_accel_data['Timestamp'].max()}\")\n",
    "print(f\"\\n总数据行数: {len(all_accel_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns, UTC]\n",
      "datetime64[ns, UTC]\n"
     ]
    }
   ],
   "source": [
    "print(all_accel_data[\"Minute\"].dtype)  # 应该是 datetime64[ns]\n",
    "print(all_accel_data[\"Timestamp\"].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据处理完成，最终数据已保存为 'final_merged_data.csv'\n",
      "清理后的数据行数: 215196\n",
      "\n",
      "各活动类型的样本数量:\n",
      "Activity Type ID\n",
      "2806    35310\n",
      "2807    22537\n",
      "2808    19519\n",
      "2809     1657\n",
      "2810    11257\n",
      "2811    16835\n",
      "2812    11879\n",
      "2813    24789\n",
      "2814    37052\n",
      "2815    34361\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 加载活动标签文件\n",
    "label_data = pd.read_csv(activity_file)\n",
    "\n",
    "# 处理标签数据的时间 (2024/9/2 6:16:00 格式)\n",
    "time_columns = [\"Started\", \"Finished\", \"Updated\"]\n",
    "for col in time_columns:\n",
    "    if col in label_data.columns:\n",
    "        # 先转换为datetime（此时是UTC+9本地时间）\n",
    "        local_time = pd.to_datetime(label_data[col], format='%Y/%m/%d %H:%M')\n",
    "        # 添加UTC+9时区信息并转换到UTC\n",
    "        label_data[col] = (\n",
    "            local_time.dt.tz_localize('+0900')  # 标记为UTC+9时区\n",
    "            .dt.tz_convert('UTC')               # 转换到UTC\n",
    "        )\n",
    "# 基于 'Started' 列进行匹配，保留加速度数据的毫秒级时间戳\n",
    "merged_data = pd.merge(\n",
    "    all_accel_data[all_accel_data[\"Minute\"].notna()],  # 只使用有效的时间戳\n",
    "    label_data[label_data[\"Started\"].notna()],         # 只使用有效的标签时间\n",
    "    how=\"left\",\n",
    "    left_on=\"Minute\",\n",
    "    right_on=\"Started\"\n",
    ")\n",
    "\n",
    " # 保存最终合并后的数据集\n",
    "# 1. 去除没有活动标签的数据\n",
    "merged_data = merged_data.dropna(subset=['Activity Type ID'])\n",
    "\n",
    "# 2. 确保 Activity Type ID 是整数类型\n",
    "merged_data['Activity Type ID'] = merged_data['Activity Type ID'].astype(int)\n",
    "# 保存最终合并后的数据集\n",
    "merged_data.to_csv(\"final_merged_data.csv\", index=False)\n",
    "\n",
    "print(\"数据处理完成，最终数据已保存为 'final_merged_data.csv'\")\n",
    "\n",
    "# 4. 打印一些统计信息\n",
    "print(f\"清理后的数据行数: {len(merged_data)}\")\n",
    "print(\"\\n各活动类型的样本数量:\")\n",
    "print(merged_data['Activity Type ID'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成清洗+标准化后剩余行数: 215195\n",
      "其中无 'Activity Type ID' 的数据已被移除。\n",
      "已将清洗+标准化后的数据保存至: cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# (2) 简单数据清洗与标准化\n",
    "###############################################################################\n",
    "# 1) 去除加速度中的缺失值（Accel_X, Accel_Y, Accel_Z）\n",
    "merged_data = merged_data.dropna(subset=[\"Accel_X\", \"Accel_Y\", \"Accel_Z\"])\n",
    "\n",
    "# 2) 过滤极端异常 (假定绝对值 > 50 属于异常)\n",
    "merged_data = merged_data[\n",
    "    (merged_data[\"Accel_X\"].abs() <= 50) &\n",
    "    (merged_data[\"Accel_Y\"].abs() <= 50) &\n",
    "    (merged_data[\"Accel_Z\"].abs() <= 50)\n",
    "]\n",
    "\n",
    "# 3) 去除没有标签的加速度数据\n",
    "merged_data = merged_data.dropna(subset=[\"Activity Type ID\"])\n",
    "merged_data[\"Activity Type ID\"] = merged_data[\"Activity Type ID\"].astype(int)\n",
    "\n",
    "# # # 5) 使用 StandardScaler 对加速度列做标准化：\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # 如果你想按每个 Subject 单独标准化，则 groupby(\"Subject\") 处理：\n",
    "# # 如果不需要分主体，也可以整体标准化\n",
    "# df_list = []\n",
    "# for subj, grp in merged_data.groupby(\"Subject\"):\n",
    "#     scaler = StandardScaler()\n",
    "#     grp[[\"Accel_X\", \"Accel_Y\", \"Accel_Z\"]] = scaler.fit_transform(\n",
    "#         grp[[\"Accel_X\", \"Accel_Y\", \"Accel_Z\"]]\n",
    "#     )\n",
    "#     df_list.append(grp)\n",
    "\n",
    "# merged_data = pd.concat(df_list, ignore_index=True)\n",
    "merged_data = merged_data.sort_values(by=\"Timestamp\").reset_index(drop=True)\n",
    "\n",
    "print(\"完成清洗+标准化后剩余行数:\", len(merged_data))\n",
    "print(\"其中无 'Activity Type ID' 的数据已被移除。\")\n",
    "# 6) 将清洗后的数据写回新的 CSV 文件\n",
    "merged_data.to_csv(\"cleaned_data.csv\", index=False)\n",
    "print(\"已将清洗+标准化后的数据保存至: cleaned_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
